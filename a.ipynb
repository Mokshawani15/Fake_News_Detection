{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Moksha's Lappy/Desktop/GFG/JAIT-V13N6-652.pdf\n"
     ]
    }
   ],
   "source": [
    "path=str(input(\"Enter the path : \"))\n",
    "a=\"\\\\\"\n",
    "b=\"/\"\n",
    "newpath=path.replace(a,b)\n",
    "print(newpath)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Detection of Fake News Using Machine Learning \\nand Natural Language Processing Algorithms  \\n \\nNoshin N irvana  Prachi , Md. Habibullah, Md. Emanul Haque  Rafi, Evan Alam , and Riasat Khan  \\nElectrical and Computer Engineering , North South University , Dhaka, Bangladesh  \\nEmail: {noshin.nirvana, md.habibullah, emanul.haque, evan.alam, riasat.khan}@northsouth.edu  \\n \\n \\n \\nAbstract—The amount of information shared on the internet, \\nprimarily via web -based networking media, is regularly  \\nincreasing . Because of the easy availability  and exponential \\nexpansion of data through social media networks, \\ndistinguishing between fake and real info rmation  is not \\nstraightforward . Most smartphone users tend to read news \\non social media  rather than on the internet. The information \\npublished on news websites often needs to  be authenticate d. \\nThe simple spread of information and news  by instant  \\nsharing ha s included the exponential growth  of its \\nmisrepresentation. So, fake news has been a major issue ever \\nsince the growth and expansion of the internet for the general \\nmass . This paper employs  several machine learning, deep \\nlearning and natural language proce ssing techniques for \\ndetecting false news, such as logistic regression, decision tree, \\nnaive bayes, support vector machine, long short -term \\nmemory, and bidirectional encoder representation from \\ntransformers. Initially, the machine learning and deep \\nlearnin g approaches are trained using an open -source fake \\nnews detection dataset to determine if the information is \\nauthentic or counterfeit . In this work, the corresponding \\nfeature vectors are generated from various feature \\nengineering methods such as regex, tok enization, stop words, \\nlemmatization  and term frequency -inverse document \\nfrequency. All the machine learning and natural language \\nprocessing models’ performance were evaluated in terms of \\naccuracy, precision, recall, F -1 score, ROC curve, etc. For the \\nmach ine learning models, logistic regression, decision tree, \\nnaive bayes, and SVM achieved classification accuracies of \\n73.75%, 89.66%, 74.19%, and 76.65%, respectively. Finally, \\nthe LSTM attained 95% accuracy, and the NLP -based BERT \\ntechnique obtained the hig hest accuracy of 98% .  \\n \\nIndex Terms —bidirectional encoder representation from \\ntransformers, fake news  detection , lemmatization,  long short -\\nterm memory,  naive Bayes , support vector machine, \\ntokenization  \\n \\nI. INTRODUCTION  \\nInformation is significant for human dynamics and \\naffects life practices. In earlier days, the daily news or \\ninformation was presented through print media, \\nnewspapers, and electronic media such as television and \\nradio. The data from these publishing technologies are \\nmore credible as it is either self -screened or constrained by \\nspecialists  [1]. These days, individ uals are presented with an extreme amount of data through various sources, \\nparticularly with the prominence of the internet and web -\\nbased media stages. The ease of internet access has caused \\nthe hazardous development of a wide range of falsehoods \\nlike malicious discussion, double -dealing, fabrications, \\nfake news, spam assessment, which diffuses quickly and \\nwidely in the human culture. The misinformation of online \\nsocial media has become a global problem in public trust  \\nand society  as it has become an essent ial mode of \\ncommunication and networking nowadays . \\nNowadays, online social platforms and blogs contain a \\nsignificant amount of fake and fabricated news, negatively \\naffecting society  [2]. This news is embellished with \\ndubious facts and misleading informatio n, causing \\ninterpersonal anxiety and detrimental social panic. This \\nunreliable information destroys people's trust and adversely \\ninfluences the economy and major political processes, such \\nas the stock market, elections, etc. The proliferation of fake \\nand f abricated news is generally detected manually by \\nhuman verification. This manual fact -checking process is \\nsubjective in nature, laborious, time -consuming , and \\ninefficient . In recent years, automatic systems based on \\nmachine learning and natural language pr ocessing \\nalgorithms have been utilized to tackle the issue of fake \\nnews detection  [3], [4]. With the advancement of \\ntechnology and artificial intelligence, these automatic \\nsystems efficiently restrain misleading and false news \\npropagation . Thus, these tech niques have created deep \\ninterest among researchers in detecting fake news for a \\nbetter future endeavor.  \\nThis paper has designed a fake news detection and \\nclassification system using different types of machine \\nlearning techniques. The open -source fake news  datasets of \\nthe proposed artificial news detection system contain the \\ninformation of various articles' authors, captions, and main \\ndescriptions. Initially, the dataset is preprocessed using \\nconventional techniques, e.g., regex, tokenization, stop \\nwords, l emmatization, and then applied NLP techniques, \\ncount vectorizer, TF -IDF vectorizer. The major \\ncontributions of this work are as follows:  \\n• In this paper, an automatic fake news detection \\nsystem has been developed using various machine \\nlearning and natural l anguage processing \\nalgorithms. This work uses logistic regression, \\ndecision tree, naive bayes, and SVM machine \\nlearning techniques.   \\nManuscript received  April 4, 2022 ; revised June 13, 2022 ; accepted \\nJuly 4, 2022 .Journal of Advances in Information Technology Vol. 13, No. 6, December 2022\\n652 doi: 10.12720/jait.13.6.652-661• Additionally, Long Short -Term Memory (LSTM), \\ndeep learning model and natural language \\nprocessing algorithm, Bidirectional E ncoder \\nRepresentations from Transformers (BERT) are \\nalso implemented.  \\n• Next, the efficiency of all the machine learning and \\nnatural language processing models are compared \\nin terms of classification accuracy, precision, recall, \\nF-1 score, and ROC curve.  \\n• Finally, the performance of the proposed fake news \\ndetection system is compared with previous \\nrelevant works in terms of classification accuracy. \\nThe nobility of this work is to utilize the BERT -\\nbased NLP model for detecting fake news.  \\nThe other part of the  paper is constructed as follows. In \\nSection III, the proposed system has been discussed with \\nappropriate equations. The actual results of the research \\nhave been shown in Section IV. Lastly, Section V \\nconcludes the paper with some directions for the future \\nimprovement of this work . \\nII. RELATED WORKS  \\nSome of the recent works implemented to detect fake and \\nfabricated news have been discussed in the following \\nsection . Machine learning and deep learning -based neural \\nnetwork models execute the identification and \\nclassification of real and fake news.  For in stance, i n [5], the \\nauthors worked on fake news detection with the help of a \\nmixed deep learning technique, CNN -LSTM. This paper \\nhas used the Fake News Challenge (FNC) dataset, which \\nwas created in 2017. They matched the claim with the news \\narticle body wh ether the claim matches with the article \\nbody or not. The authors have developed four data models. \\nFirst, they use data without preprocessing, second with \\npreprocessing. The authors obtained different results when \\nthey preprocessed data and when they did n ot. The third and \\nfourth models are built on dimensionality reduction \\ntechniques by using PCA and Chi -square approaches . \\nFinally, they trained on forty -nine thousand and nine \\nhundred seventy -two samples and tested on twenty -five \\nthousand and four hundred t hirteen headlines and articles \\nby CNN -LSTM. On their model with no knowledge of \\ncleanup or preprocessing, the achieved accuracy was 78%. \\nWhen preprocessing was done, the accuracy increased up \\nto 93%. Next, the application of Chi -square raises the \\naccuracy by 95%. Lastly, they conclude that using PCA \\nwith CNN and LSTM design resulted in the highest \\naccuracy of 96%, significantly reducing the prediction time . \\nIn [6], T. Jiang et al.  used baseline fake news identification \\ntechniques to locate the baseline meth ods' flaws and \\nprovide a viable alternative. First, the authors performed \\nfive completely different  conventional  machine learning \\nmodels and three deep learning models to compare their \\nefficiency. The authors used two datasets (ISOT and \\nKDnugget) of variou s sizes to test the corresponding \\nmodels' performance in this work. Finally, they take \\nadvantage of an adaptation of modified McNemar's check \\nto decide if they are square measure contrasts between these \\ntwo models' presentation, then determine the simplest  \\nmodel for detecting the fake news. The authors obtained  accuracies of  approximately  99.94%  and 96.05%  on the \\nISOT dataset and KDnugget dataset , respectively . \\nIn a recent work [ 7], the authors designed a system for \\ndetecting fake news using various machine  learning \\ntechniques. First, each tweet/post has been categorized as a \\nbinary categorization result by the authors. They collected \\ndata manually from their own research sets by using Twitter \\nAPI and the DMOZ directory. The authors ran a test of their \\npropo sed system on the Twitter dataset. The results show \\nthat fifteen percent of fabricated tweets and forty -five \\npercent of the actual tweets were adequately classified, and \\nthe remainder of the posts were not decided. In th is paper, \\nthe author proposed the de tection of deception using the \\nlabeled benchmark dataset “LIAR ”. They have also \\nimproved efficiency in the detection of fake posts/news \\nwith evidence. The authors have introduced the need for \\nhoax detection in their system. They used the ML approach \\nby combining news content and social content. Finally, the \\nauthors cla im their proposed system's performance is good \\ncompared to other works described in the literature . In [8], \\nA. Jain et al.  design an automated system that detects the \\nnews as false or true. Sometimes, social media like \\nFacebook, YouTube, Twitter, and other  online platforms \\nspread the news, creating anxiety and unrest in society. In \\nthis paper, the author applied several machine learning -\\nbased fake news detection systems. The author utilized \\nnaive bayes classifier, SVM algorithm and logistic \\nregression in th eir proposed detection system. They \\nimplement their model and classify the authentic and \\nfabricated news. Finally, the proposed SVM model \\nachieved an accuracy of 93.5% . Machine learning \\nensemble technique s have been used in [ 9] to detect and \\nclassify fake news automatically. In this regard, the textual \\nfeatures have been applied in different machine learning \\napproaches. This paper used ISOT and two open -source \\ndatasets to build the proposed system. In the data \\npreprocessing step, documents containing less t han 20 \\nwords are filtered out. Next, the dialectal mechanism LIWC \\nis employed to convert the textual features into numerical \\nvalues. Next, various machine learning algorithms, logistic \\nregression, SVM, KNN, random forest and boosting \\nclassifies have been u sed. Finally, the decision tree \\napproach with 10 -fold cross -validation obtained the highest \\naccuracy of 94%.  \\nIII. METHODOLOGY  \\nIn this section, we have discussed the methodology of \\nour work  in great detail . We have explained  all the  regular  \\nmachine learning , neu ral network  and NLP methods that \\nwe have used in our dataset.  \\nA. Dataset  \\nIn this work, an open -source  fake news  dataset from \\nKaggle [ 10] has been used. The public dataset has been \\ncreated by web scrapping of different search engines.  Lots \\nof fake news and agenda always take place around us, so \\nthe whole data was curated with the help of automated data \\nscience technologies. It was posted on the data science \\ncommunity as a challenge to use those data to implement \\nefficient fake news detection architecture.  This specific \\nJournal of Advances in Information Technology Vol. 13, No. 6, December 2022\\n653database of fake news has been utilized in this work \\nbecause it involves a diverse dataset from a wide variety of \\nnews portals and social sites. The dataset comprises 26,000 \\nunique sample documents and has been used successfully \\nin some papers to identify fake news  [11], [12]. The original \\ndataset has four columns, viz. id, title, author, text. The id \\ncolumn represents a particular numerical label  for a news \\narticle; the title holds the heading of a news article; the \\nauthor column contains the i nformation about the writer  of \\nthe news item; and finally, under the text column, the text \\nof the report  has been described . The training dataset has \\nthe label column, which marks the news item as potentially \\nunreliable or reliable. It is worth mentioning that, the \\ndataset has 20,822 unique values in the text column . \\nB. Data Preprocessing  \\nWe need to transform the text data using preprocessing \\ntechniques, NLP, tokenization, and lemmatization before \\nfeeding them through the ML and DL models  [13].  Data \\npreprocess ing helps to remove the noises and inconsistency \\nof data, which increases the performance and efficiency of \\nthe model. In this work, we have used traditional techniques, \\nregex, tokenization, stopwords, lemmatization, NLP \\ntechnique, and TF -IDF for data prep rocessing. The \\nimplemented data preprocessing techniques are explained \\nbriefly in the subsequent paragraphs . \\n1) Regex  \\nWe use regex to remove punctuations from the text data. \\nOften in the sentences, there may have extra punctuations \\nlike exclamatory signs. We use regex to remove those \\nadditional  punctuations to make the dataset noise -free. \\nRegex  is based on context -free gr ammar.  \\n2) Tokenization  \\nTokenization, preprocessing tool is used to break the \\nsentences into words  [14]. \\n3) Stopwords \\nWe use the English stopwords library in our \\npreprocessing  technique because our model data is English . \\nWe need to use the stopwords preprocessing  technique to \\nremove noises , make the model faster and more efficient, \\nand save  memory space.  \\n4) Lemmatization  \\nLemmatization is used to transform the words into root \\nwords. We can resolve data ambiguity  and inflection  with \\nlemmatization.  \\n5) NLP technique  \\nNLP tec hniques  have been applied  to convert the texts \\ninto meaningful numbers to feed these numbers into our \\nproposed machine learning algorithm.  \\n6) Bag of words  \\nThe b ag of words technique converts texts into machine -\\nunderstandable numbers , which is expressed as:  \\n𝑇𝐹−𝐼𝐷𝐹 =𝑇𝐹𝑡𝑑.𝐼𝐷𝐹 𝑡                      (1) \\nwhere 𝑡 is a term , and 𝑑 denotes  the document s. TF stands \\nfor term frequency, which is a measurement of how \\nfrequently a term appears in a document. Consequently, \\nterm frequency 𝑇𝐹 is measured as:  \\n𝑇𝐹=𝑞𝑡𝑑\\n𝑁𝑢𝑚𝑏𝑒𝑟  𝑜𝑓 𝑡𝑒𝑟𝑚𝑠  𝑖𝑛 𝑡ℎ𝑒 𝑑𝑜𝑐𝑢𝑚𝑒𝑛𝑡                (2) where 𝑞 is the n umber of times the term, 𝑡 appears in the \\ndocument, 𝑑.𝐼𝐷𝐹 denotes inverse document frequency, \\nwhich indicates the importance of a particular term . IDF is \\ncalculated as:  \\n𝐼𝐷𝐹 = log(1+𝑛)\\n(1+𝑑𝑓)𝑑𝑡+1                             (3) \\nwhere 𝑛 means the number of documents and the \\ndenominator indicates the document frequency of the term, \\n𝑡. \\nC. Machine Learning Algorithms  \\nTo detect and classify  real and  fake news, we have used \\ndifferent machine learning algorithm s: logistic regression , \\nnaive bayes , decision tree, and support  vector machine.  \\n1) Logistic regression  \\nLogistic regression is a statistical ML classification \\nmodel  [15]. The basis of the proposed system consists of \\nthe binary classification problem. Logistic regression is \\nmanipulated to model the probability of a certain existing \\nevent, such as true/false, reliable/unreliable, win/lose, etc. \\nHence, the logistic model is one of the most appropriate \\nmodels for the fake news detection system . The condition \\nfor predicting logistic model is : \\n0≤ℎ𝜃(𝑥)≤1                              (4) \\nThe logistic regression sigmoid function is  expressed as:  \\nℎ𝜃(𝑥)=𝑔(𝜃𝑇𝑋)                                (5) \\nwhere, \\n𝑔(𝑧)= 1\\n(1+𝑥−𝑧)                                  (6) \\nand the cost function of logistic regression is:  \\n   𝐽(𝜃)=1/𝑚∑ 𝑐𝑜𝑠𝑡 (ℎ𝜃(𝑥𝑖 𝑚\\n𝑖=1 ,𝑦𝑖))             (7) \\n2) Naive  Bayes  \\nThe naï ve Bayes method is at the basis of Bayesian \\nclassifiers. It is a strategy for looking at possible outcomes \\nthat allow flip ping the state around straightforwardl y [16]. \\nA conditional probability is a probability that incident X \\nwill happen provided information Y. The typical notation \\nfor this is 𝑃(𝑋|𝑌). We can use the naive bayes rule  to \\ncompute this probability when we only have the probability \\nof the opposite result and the two components separately . \\n𝑃(𝑋|𝑌)=𝑃(𝑋) 𝑃(𝑌|𝑋)\\n𝑃(𝑌|𝑋) 𝑃(𝑌)                             (8) \\nThis restatement can be extremely useful when we  are \\ntrying to predict the likelihood of something based on \\nexamples of it  is happening . \\nIn this research , we are attempting to determine if an \\narticle is false or genuine based on its contents. We may \\nrephrase it in terms of the likelihood of that document being \\nreal or fake if it has been predetermined to be real or fake . \\nThis condition  is useful sinc e we already have instances of \\nreal and fake articles in our data collection.  \\nGenerally, a large assumption is considered for \\ncomput ing the likelihood of the article happening; it is \\nequal to the product of the probabilities of each word inside \\nits occurre nce, making this procedure a “na ive” Bayesian \\none [17]. This assumption suggests that there is no \\nJournal of Advances in Information Technology Vol. 13, No. 6, December 2022\\n654connection between the two words. It is also known as the \\nassumption of independence. We can estimate the \\nlikelihood of a term occurring by looking at a set o f real and \\nfake article samples and noting how many times it appears \\nin each class. The necessity for training the pre -classified \\nsamples distinguishes this method from the typical \\nsupervised learning . \\n3) Decision tree \\nThe conventional J48 method is one of the most widely \\nused classification algorithms  [18]. It is based on the C4.5 \\nalgorithm, which requires all data to be studied \\nquantitatively and categorically. As a result, continuous \\ndata will not be investigated [ 19]. J48 technique employs \\ntwo distinct p runing techniques.  \\n \\nAlgorithm  1. Algorithm of the  proposed  decision \\ntree classification m odel \\nInput : Predefined classes  with 17,000 number of \\nfeatures.  \\nOutput : decision tree construction.  \\nBegin  \\nStep 1: Make the tree ’s root node.  \\nStep 2:  \\n   Return leaf node ‘positive’ if all instances are positive.  \\n   Return leaf node ‘negative’ if all instances are \\nnegative.  \\nStep 3: Determine the current state ’s entropy 𝐻. (S) \\nStep 4: Calculate the entropy for each characteristic.  \\nStep 5: Choose the attribute with the highest IG value \\n(𝑆,𝑥) \\nStep 6: From the list of attributes, remove the attribute \\nwith the greatest IG.  \\nStep 7: Continue until all characteristics have been \\nexhausted or the decision tree has all leaf nodes . \\nEnd \\n \\nAlgorithm 1 briefly explains the bui lding steps of the \\ndecision tree classification technique. The first approach is \\nsubtree replacement, which refers to replacing nodes in a \\ndecision tree's leaves to reduce the number of tests in the \\nconvinced route. In most cases, subtree raising has a min or \\ninfluence on decision tree models. Usually, there is no \\naccurate method to forecast an option ’s usefulness. \\nHowever, turning it off may be advisable if the induction \\noperation takes longer than expected because the subtree's \\nraising is computationally c omplex. Next, the current \\nstate's entropy and its corresponding characteristics are \\ndetermined. Consequently, the attribute with the maximum \\ninformation gain is computed and removed. This process is \\ncontinued until all features have been exhausted or the \\ndecision tree has all leaf nodes . \\n4) Support Vector Machine (SVM)  \\nSVM, which is also known as support vector machine \\nnetwork, is a supervised learning method  [20]. SVMs are \\ntrained using particular data that has previously been \\ndivided into two groups  [21]. As a result, once the model \\nhas been trained, it is created. Moreover, the goal of the \\nsupport vector machine technique is to decide any new \\ninformation belongs to which group and to increase the \\nclass label  [22]. The final goal of the SVM is to locate a \\nsubspace that divides the data into two parts. As Radial Basis Function (RBF) is suitable for large systems like a \\ncollection of media articles, it was chosen as the kernel for \\nthis proposed system . On two samples 𝑥 and 𝑥′, the radial \\nbasis function is  expressed as:  \\n𝐾(𝑥,𝑥′)=𝑒−‖𝑥−𝑥′‖2\\n2𝜎2                       (9) \\nwhere ‖𝑥−𝑥′‖2 is a free parameter that denotes the \\nsquared Euclidean distance.  \\nD. Deep Learning  and Natural Language Processing \\nAlgorithms  \\nIn this work, we have used a deep learning te chnique, \\nLSTM, and an NLP algorithm, BERT, to classify fake news, \\nand both of them are dynamic . \\n1) Long Short -Term Me mory (LSTM)  \\nLong Short -Term Memory (LSTM) is an exclusive type \\nof recurrent neural network, which allows information to \\nendure. Typical RNN networks encounter short -term \\nmemory, which is solved by the cell states of the LSTM. \\nSeparate hidden motors are used in LSTMs, and their \\nnature is to recall inputs for a long time [ 23]. A memory \\ncell, also known as a gated leaky neuron or an accumulator, \\nhas a relationship in the following stages with its weight of \\n1. It mimics its genuine position and inserts an external \\nsignal, but this signal is multiplied by another unit that \\ndetermines when to wipe or keep information from memory. \\nFinally, the sigmoid  layer -based forget gates control the \\ntransfer of data to the following hidden networks. Fig. 1 \\nshows a generic LSTM based neural network architecture . \\n \\nFigure 1. Generic LSTM architecture . \\nFor our classification, we used an LSTM model with an \\ninput layer  that takes the input titles and article body and an \\nembedding layer that turns every word into a 300 -pixel \\nvector. As there are 256 features, this layer will produce a \\n256×300 matrix. The weights we obtain from matrix \\nmultiplication will be in the output matrix, which will \\ngenerate a vector for every word. These vectors are input \\nthrough an LSTM, which is subsequently transferred to a \\nfully linked dense layer, resulting in a single final output. \\nJournal of Advances in Information Technology Vol. 13, No. 6, December 2022\\n655Table I  shows the model layers and parameters, which were \\ntrained on batches of size 256 . \\nTABLE I.  LAYERS AND PARAMETERS OF THE PROPOSED LSTM  \\nMODEL  \\nLayer  Output Shape  Number of \\nParameters  \\nInput  (None, 256)  0 \\nEmbedding  (None, 256, 300)  60,974,100  \\nSpatial Dropout  (None, 256, 300)  0 \\nBidirectional  (None, 256)  439,296  \\nDense  (None, 64)  16,448  \\nDropout  (None, 64)  0 \\nTotal parameters: 61,429,909  \\nTrainable parameters: 61,429,909  \\nNon-trainable parameters: 0 \\n \\n2) Bidirectional Encoder Representation from \\nTransformers (BERT)  \\nBERT is des cribed  to be pre -trained bidirectional \\nrepresentations from an unlabeled text by conditioning both \\nright and left backgrounds in all levels [ 24]. As a result, the \\nBERT model might suffice with only one additional output \\nlayer to produce advanced models for vario us tasks, \\nincluding query answers. BERT is composed of two \\ncomponents, encoder and decoder. In this pre -training \\nphase, this model learns about the language and its \\ncorresponding contexts. As this technique learns contexts \\nfrom both directions simultaneous ly, the co ntexts of words \\nare better learned . \\nFor tokenizing sentences into words, converting token \\nstrings to ids and back, and encoding/decoding, \\nBertTokenizer from the pretrained ‘bert -base-uncased ’ \\nmodel , was utilized in this study. The max sentence le ngth \\nis 60 characters, and we utilized the encode plus technique \\nto encode each one  of them . This technique will tokenize \\nthe phrase, prep the [CLS] (classification) token at the \\nbeginning, and append the [SEP] , which tells BERT where \\nto start the next phr ase. In most cases, it is inserted a fter \\neach phrase ’s token . Tokens should be mapped to their ids; \\nthe phrase should be padded to the attention masks , and the \\nmaximum length  for [PAD] (padding) tokens should be \\ncreated. The BERT model uses the argument of  attention \\nmask, which specifies which tokens should be dealt with \\nand which can be ignored . Finally, in this step, the model is \\nnotified whether tokens include valid data or not . The \\narchitecture of the BERT model  employed in this proposed \\ndetection system has been depicted in Fig. 2. \\n \\nFigure 2. Architecture  of BERT technique . Table II  shows the layers and parameters of the proposed  \\nBERT model input ids  and attention masks used as the \\ninput layer. After that, the output of the input layers goes to \\nthe transformer BERT model, which is subsequently \\ntransferred to a fully linked dense layer, resulting in a single \\nfinal output . \\nTABLE II.  LAYERS AND PARAMETERS OF THE PROPOSED BERT-\\nBASED NLP MODEL  \\nLayer  Number of \\nParameters  Connected to  \\nInput  0  \\nAttention masks  0  \\nTF BERT model  109,482,240  Input [0][0]  \\nAttention masks \\n[0][0]  \\nDense  24,608  TF BERT model \\n[0][1]  \\nDropout  0 Dense [ 0][0] \\nTotal parameters: 109,506,881 \\nTrainable parameters: 109,506,881  \\nNon-trainable parameters: 0  \\nIV. RESULT AND ANALYSIS  \\nThis section discusses the numerical results of the \\nproposed fake news detection system with the applied  \\nregular  ML, DL, and NLP approaches.  The employed fake \\nnews dataset has been divided into 8:2 training and testing \\nsamples. After completion of the necessary processing and \\ntraining of the dataset, all the models are assessed. In this \\nwork, all the models are evaluated in various ways by \\nchecking their accuracy, confusion matrix, recall, precision, \\nF1-score, ROC curve, and other metrics . \\nA. Performance of Logistic Regression Model  \\nIn Fig. 3, the confusion matrix for the logistic regression \\nmodel of the proposed system has been shown. The real \\nnews class has 862 right prediction s and 170 wrong \\nprediction s from 1032 test samples of real news. Therefore , \\nthe accuracy for real news prediction is 83.52% , and for the \\nfake news class, it h as 487 correct prediction s but a \\nsignificant number of  the wrong prediction s of 310 from \\n797 test samples . So, the accuracy for fake news is 61%  and \\nfinally, t he overall accuracy is 74%.  \\n \\nFigure 3. Confusion matrix for logistic regression . \\nJournal of Advances in Information Technology Vol. 13, No. 6, December 2022\\n656 \\nFigure 4. ROC curve for logistic regression . \\nAccording to Fig. 4, the area under the curve (AUC) \\nscore of the ROC curve of the proposed logistic regression \\nalgorithm is 0.79. T he rest of the performance metrics for \\nthe logistic regression model are demonstrated  in Table III . \\nThe pr oposed logistic reg ression model's precision, recall, \\nand F1 -score are 74%, 72%, and 73%,  respectively .  \\nTABLE III.  LOGISTIC REGRESSION MODEL ’S PERFORMANCE METRICS  \\n Precision  Recall  F1-score  \\n0 (Not Fake)  0.74 0.84 0.78 \\n1 (Fake)  0.74 0.61 0.67 \\nAccuracy    0.74 \\nWeighted \\nAverage  0.74 0.74 0.73 \\nB. Performance of Na ive Bayes Model  \\nThe confusion matrix for the naive bayes  model of the \\nproposed system has been shown  in Fig. 5. The authentic  \\nnews class has 830 right prediction s and 202 wrong \\nprediction s from  the total  1032 test samples. So, the \\naccuracy for real news prediction is 80%, and for the fake \\nnews class, it has a significant number of wrong \\nclassifications similar to the logistic regression model. \\nFinally, the accuracy for fake news is 66%, and the overall \\naccuracy is 74%.  \\n \\nFigure 5. Confusion matrix  for naive  bayes . \\nThe true and false positive rates of the proposed naive \\nbayes approach are depicted in Fig. 6. According to Fig. 6, \\nthe naive bayes model has an ROC AUC score of 0.79. In \\nTable IV, the rest of the performance metrics for the naive \\nbayes model are demonstra ted. The precision, recall, and F1-score of the proposed naive bayes model are 74%, 73%, \\nand 73%, respectively.  \\n \\nFigure 6. ROC curve for naive bayes . \\nTABLE IV.  VARIOUS EVALUATION METRICS OF THE NAIVE BAYES \\nMODEL  \\n Precision  Recall  F1-score  \\n0 (Not Fake)  0.75 0.80 0.78 \\n1 (Fake)  0.72 0.66 0.69 \\nAccuracy    0.74 \\nWeighted \\nAverage  0.74 0.74 0.74 \\nC. Performance of Decision Tree Model  \\nIn Fig. 7, the confusion matrix for the decision tree \\nmodel of the proposed system has been demonstrated . The \\nreal news class has 940 right prediction s and 92 wrong \\nprediction s from 1032 test samples of real news. So, the \\naccuracy for real news prediction is 91%, a nd for the fake \\nnews class, it has 700 correct prediction s but a n acceptable \\nnumber of  the wrong prediction s of 97 from  797 test \\nsamples of fake news. So, the accuracy for fake news is \\n88%. Finally, t he decision tree technique achieved an  \\noverall accuracy  of 90%. \\n \\nFigure 7. Confusion matrix for decision tree.  \\nAccording to Fig. 8, the ROC AUC value of the \\nproposed decision tree algorithm is 0.89. In Table V, the \\nrest of the performance metrics for the decision tree model \\nare demonstrated. The precision, recall, and F1 -score of \\nthe proposed decision tree model are 90%, 89%, and 89%, \\nrespectively.  \\nJournal of Advances in Information Technology Vol. 13, No. 6, December 2022\\n657 \\nFigure 8. ROC  curve for decision tree . \\nTABLE V.  DECISION TREE MODEL ACCURACY METRICS  \\n Precision  Recall  F1-score  \\n0 (Not Fake)  0.91 0.91 0.91 \\n1 (Fake)  0.88 0.88 0.88 \\nAccuracy    0.90 \\nWeighted \\nAverage  0.90 0.90 0.90 \\nD. Performance of Support Vector Machine Model  \\nIn Fig. 9, the confusion matrix for the SVM model  with \\nthe RBF kernel  of the proposed system has been shown.  \\nThe accuracies of the real and fake news are 82% and 70%, \\nrespectively. Finally, th e overall accuracy  of the SVM \\nclassifier model  is 77% . \\n \\nFigure 9. Confusion matrix f or SVM . \\n \\nFigure 10. ROC curve  for SVM . \\nAccording to Fig. 1 0, the ROC AUC coefficient of the \\nproposed SVM algorithm is 0.83. Table VI  depicts  the rest \\nof the performance metrics for the SVM  model.  TABLE VI.  SVM  MODEL  ACCURACY METRICS  \\n Precision  Recall  F1-score  \\n0 (Not Fake)  0.78 0.82 0.80 \\n1 (Fake)  0.75 0.70 0.72 \\nAccuracy    0.77 \\nWeighted Average  0.77 0.77 0.77 \\nE. Performance of LSTM Model  \\nFig. 1 1 illustrates the confusion matrix for the deep \\nlearning -based LSTM model of the proposed system . The \\nreal news class has 1920 right prediction s and 157 wrong \\nprediction s. So, the accuracy for real news prediction is \\n92%, and for the fake news class, the pr ediction is \\nsignificantly improved compared to other ML techniques. \\nFinally, the overall accuracy of the LSTM technique is 95%. \\nThe total number of test samples for each class is different \\nfrom the ML approaches because of the better \\npreprocessing for NLP methods which helps to decrease the \\nchances of removing samples . \\n \\nFigure 11. Confusion matri x for LSTM . \\nAccording to  Table VII , other  performance metrics for \\nthe LSTM model demonstrated better results . The precision, \\nrecall , and F1 -score of the proposed LSTM model are 94%, \\n95%, and 94% , respectively.  \\nTABLE VII.  PERFORMANCE METRICS OF THE LSTM  APPROACH  \\n Precision  Recall  F1-score  \\n0 (Not Fake)  0.98 0.92 0.95 \\n1 (Fake)  0.91 0.97 0.94 \\nAccuracy    0.95 \\nWeighted Average  0.95 0.95 0.95 \\n \\nFigure 12. Accuracy and loss vs. epochs graph of LSTM.  \\nJournal of Advances in Information Technology Vol. 13, No. 6, December 2022\\n658Fig. 12 shows the accuracy and loss vs. epochs graphs of \\nLSTM with respect to epoch. For the LSTM model, initially, \\nthe model’s validation accuracy was 95%, which did not \\nvary significantly with the change of the epochs.  \\nF. Performance of BERT  Model  \\nIn this section, the results for the proposed fake news \\ndetection system implemented on the BERT technique will \\nbe discussed. Table VIII  shows the encoder and decoder \\nresult on an examp le sentence. The purpose of this result is \\nto show how all of the input sentences are encoded and \\ndecoded. Here the input for the encoding is “Hi nice meet \\nyou!”. After encoding, we can see that all the words and \\nsymbols represent a value, i.e., “hi” is assigned a numerical \\nvalue of 7632. If we decode it, we will get the exact output \\ngiven to the encoder as an input. There are two new words \\nafter decoding. One is at the beginning of the sentence, \\nwhich is CLS, which represents classification. Another one \\nis SEP at the end of the sentence, which tells BERT where \\nto start the following sentence . \\nTABLE VIII.  ENCODER AND DECODER EXAMPLE RESULTS  \\nInput  encode = bert_tokenizer.encode (“Hi nice meet you !”)  \\ndecode = bert_tokenizer.decode (encode)  \\nCommand  print (“Encode: X”, en code)  \\nprint (“Decode: X”, decode)  \\nOutput  Encode: [101, 7632, 3835, 3113, 2017, 999, 102]  \\n            Decode: [CLS] hi nice meet you! [SEP]  \\n \\n \\nFigure 13. Accuracy and loss vs. epochs graph s of BERT  framework . \\nFig. 13 shows the accuracy and loss graph of BERT with \\nrespect to epoch. For the BERT model, at the initial stages \\nof training, the model's validation starts from 97%, which \\ndid not change remarkably, and after three epochs, it \\nincreased only by 1% and achieved 9 8%. \\nG. Model Comparison of Our Paper  \\nIn Table IX, comparison for all the applied detection \\nmodels have been demonstrated that we have trained in this \\nwork. For the machine learning -based techniques, the fake \\nnews detection performs well for the decision tree classifier, \\nbut the naive bayes and logistic re gression approaches \\nperform unsatisfactorily. The highest accuracy from \\nmachine learning models is 90% for the decision tree \\napproach. The deep learning LSTM approach achieved the \\nsecond -highest accuracy of 95%. Finally, the best detection performance is o ffered by the NLP -based BERT technique, \\nwith 98% accuracy . \\nTABLE IX.  ACCURACY COMPARISON OF DIFFERENT APPLIED \\nTECHNIQUES  \\nModels  Precision  Recall  F1-Score  Accuracy  \\nLogistic \\nRegression  74% 72% 73% 74% \\nNaive Bayes 74% 73% 73% 74% \\nDecision Tree  90% 89% 89% 90% \\nSVM  76% 76% 76% 77% \\nLSTM  94% 95% 94% 95% \\nBERT     98% \\nH. Model Comparison with  Others Work  \\nFinally, the proposed fake news detection system with \\nthe BERT technique has been compared with other related \\nworks. According to Table X, the implemented BERT \\napproach outperformed all the other works in terms of \\naccuracy .  \\nTABLE X.  ROPOSED MODEL ’S ACCURACY COMPAR ISON WITH \\nRELATED WORKS  \\nReference  Applied Metho d Accuracy  \\n[3] Random forest  95% \\n[4] Decision tree  96.8%  \\n[5] CNN+LSTM with \\nPCA  96% \\n[8] SVM  93.5%  \\n[9] Decision tree  94% \\n[25] Deep neural \\nnetwork  94% \\nOur study  BERT  98% \\nV. CONCLUSION  \\nFinding the accuracy and credibility of information and \\nnews that is available on the internet is critical nowadays. \\nIt has recently been discovered that various online \\nplatforms significantly influence disseminating misleading \\ninformation and spreading fa ke news to serve several \\ndreadful purposes and benefit many people. Because of the \\nplethora of spreading and sharing data on the internet, there \\nis a growing demand for automated false news \\nidentification systems that are accurate and efficient . This \\npaper  proposes an automatic fake news detection system \\nthat utilizes various regular machine learning, deep \\nlearning, and natural language processing techniques. \\nVarious feature extraction methods, such as regex, \\ntokenization, stopwords, lemmatization, NLP, TF -IDF, \\nwere used to preprocess the data in this suggested system. \\nNext, several models, logistic regression, decision tree, \\nnaive bayes, support vector machine, long short -term \\nmemory, bidirectional encoder representation from \\ntransformers have been employed  to classify the fabricated \\nnews. For the machine learning model logistic regression, \\ndecision tree, naive bayes, and SVM, we got 73.75%, \\n89.66%, 74.19%, and 76.65% accuracies, respectively. \\nFinally, substantial better performance was achieved by the \\nneura l network  LSTM and NLP -based BERT techniques. \\nIn the future, the proposed system can be extended to detect \\nmore specific false news  with various categories , e.g., \\nreligious, political, COVID -19, etc. The word2vec \\napproach can be applied to deal with and cl assify images \\nJournal of Advances in Information Technology Vol. 13, No. 6, December 2022\\n659and video -related visual datasets. News data from diverse \\nlanguages can be utilized to identify false news from \\ndifferent nations and countries.  A future extension of this \\nwork can be to employ attention -based deep learning \\napproaches.  \\nCONFLI CT OF INTEREST  \\nThe authors declare no conflict of interest . \\nAUTHOR CONTRIBUTIONS  \\nM. E. H. Rafi proposed the research idea; N . N. Prachi  \\nand MH  conducted the research; E. Alam  and R . Khan  \\nanalyzed the data; N. N. Prachi , M. Habibullah  and M. E. \\nH. Rafi wrote the paper; R. Khan  helped to draft the final \\nmanuscript ; all authors had approved the final version.  \\nREFERENCES  \\n[1] J. Strö mbä ck , Y. Tsfati, H . Boomgaarden , et al. , “News media trust \\nand its impact on media use: Toward a framework for future \\nresearch ,” Annals of the International Communication Association , \\nvol. 44, pp. 139-156, 2020 . \\n[2] E. Mitchelstein  and P. J. Boczkowski , “Online news consumption \\nresearch: An assessment of past work and an agenda for the future ,” \\nNew Media & Society , vol. 12, pp. 1085 -1102 , 2010 .  \\n[3] P. Henrique , A. Faustini and  T. F.  Covõ es , “Fake news detection in \\nmultiple platforms and languages ,” Expert Systems with \\nApplications , vol. 158, pp. 1-9, 2020 . \\n[4] F. A. Ozbay  and B. Alatas , “Fake news detection within online \\nsocial media using supervised artificial intelligence algorithms ,” \\nPhysica A: Statistical Mechanics and its Applications , vol. 540, pp. \\n1-19, 2020 . \\n[5] M. Umer , “Fake news stance detection using deep learning  \\narchitecture (CNN -LSTM) ,” IEEE Access , vol. 8, pp.  156695 -\\n156706 , 2020 . \\n[6] T. Jiang , J. P. Li, A . U. Haq, et al. , “A novel stacking approach for \\naccurate detection of fake news ,” IEEE Access , vol. 9, pp. 22626 -\\n22639 , 2021 . \\n[7] S. I. Manzoor, J. Singla , and Nikita , “Fake news detection using \\nmachine learn ing approaches: A systematic review ,” in Proc.  \\nInternational Conference on Trends in Electronics and Informatics , \\n2019, pp. 230 -234. \\n[8] A. Jain, A. Shakya, H. Khatter , et al. , “A smart system for fake news \\ndetection using machine learning ,” in Proc.  International \\nConference on Issues and Challenges in Intelligent Computing \\nTechniques , 2019, pp. 1 -4. \\n[9] I. Ahmad, M. Yousaf, S. Yousaf , et al. , “Fake news detection using \\nmachine learni ng ensemble methods ,” Complexity , pp. 1-11, 2020 . \\n[10] UTK machine learning club . (July 2017). Fake news , version 1. \\n[Online]. Available:  https://www.kaggle.com/c/fake -news/data  \\n[11] H. Ali, M. S. Khan, A . AlGhadhban , et al. , “All your fake detector \\nare belong to us: Evaluating adversarial robustness of fake -news \\ndetectors under black -box settings ,” IEEE Access , vol. 9, pp. 81678 -\\n81692 , 2021 . \\n[12] I. K, Sastrawan, I. P. A. Bayupati , and D. M. S.  Arsa, “Detection of \\nfake news using deep learning CNN -RNN based methods ,” ICT \\nExpress , pp. 1-13, 2021 . \\n[13] Y. A. Solangi , Z. A. Solangi, S . Aarain , et al. , “Review on Natural \\nLanguage Processing (NLP) and its toolkits for opinion mining and \\nsentiment analysis ,” in Proc.  International Conference on \\nEngineering Technologies and Applied Sciences , 2018,  pp. 1 -4. \\n[14] G. Kim and S. H. Lee, “Comparison of Korean preprocessing \\nperformance accor ding to Tokenizer in NMT transformer model ,” \\nJournal of Advances in Information Technology , vol. 11, pp. 228 -\\n232, 2020 . \\n[15] T. Daghistani and R. Alshamm ari, “Comparison of  statistical \\nlogistic regression and random  forest machine learning techniques \\nin predicting diabete s,” Journal of Advances in Information \\nTechnology , vol. 11, pp. 78 -83, 2020 . [16] W. He, Y. He, B. Li , et al. , “A naive -Bayes -based fault diagnosis \\napproach for analog circuit by using image -oriented  feature \\nextraction and selection technique ,” IEEE Access , vol. 8, pp. 5065 -\\n5079, 2020 . \\n[17] Q. Xue, Y. Zhu , and J. Wang, “Joint distribution estimation and \\nnaï ve bayes classif ication under local differential privacy ,” IEEE \\nTransactions on Emerging Topics in Computing , vol. 9, pp. 2053 -\\n2063, 2021 . \\n[18] H. A. Maddah, “Decision trees based performance analysis for \\ninfluence of sensitizers characteristics in dye -sensitized solar cells ,” \\nJournal of Advances in Information Technology , vol. 13, pp. 271 -\\n276, 2022 . \\n[19] I. D. Mienye, Y. Sun , and Z. Wang, “Prediction performance of \\nimproved decision tree -based algorithms: A review,” Procedia \\nManufacturing , vol. 35, pp. 698 -703, 2019 . \\n[20] J. A. C . Moreano and N. B. L. S. Palomino, “Global facial \\nrecognition using gabor wavelet, support vector machines and 3D \\nface models ,” Journal of Advances in Information Technology , vol. \\n11, pp. 143 -148, 2020 . \\n[21] A. B. Gumelar, A. Yogatama, D. P. Adi, et al. , “Forwa rd feature \\nselection for toxic speech classification using support vector \\nmachine and random forest,” International Journal of Artificial \\nIntelligence , vol. 11, pp. 717 -726, 2022 . \\n[22] J. Cervantes, F. Garcí a -Lamont, L. Rodrí guez , et al. , “A \\ncomprehensive survey on support vector machine classification: \\nApplications, challenges and trends,” Neurocomputing , vol. 408, pp. \\n189-215, 2020 . \\n[23] I. Benchaji, S. Douzi , and B. E. Ouahidi , “Credit card fraud \\ndetection model based o n LSTM recurrent neur al networks, ” \\nJournal of Advances in Information Technology , vol. 12, pp. 113 -\\n118, 2021 . \\n[24] N. Yadav and A. K. Singh, “Bi -directional encoder representation \\nof transformer model for sequential music recommender system,” \\nin Proc. Forum for Information Retrieval Evaluation , 2020,  pp. 49 -\\n53. \\n[25] S. Ni, J. Li , and H. Y. Kao, “MVAN: Multi -view attention n etworks \\nfor fake news detection on social media,” IEEE Access , vol. 9, pp. \\n106907 -106917, 2021 .  \\n \\nCopyright © 20 22 by the authors. This is an open access article \\ndistributed under the Creative Commons Attribution License ( CC BY -\\nNC-ND 4.0 ), which permits use, distribution and reproduction in any \\nmedium, provided that the article is properly cited, the use is non -\\ncommercial and no modifications or adaptations are made.  \\n \\nNoshin Nirvana Prachi  obtained her bachelor's degree in computer \\nscience and engineering in July 2021 from North South University, \\nBangladesh. Noshin was born in Dhaka, Bangladesh. One of her research \\nwork on deep learning -based spea ker recognition system was published \\nat Interdisciplinary Research in Technology and Management (IRTM) \\nconference. She is working on data science, machine learning, computer \\nvision and software engineering . \\n \\nMd. Habibullah  completed his B.Sc.  degree in com puter science and \\nengineering in 2021 from North South University, Bangladesh's electrical \\nand computer engineering  department.  Recently he has published a \\nmanuscript on a deep learning -based speaker recognition system at an \\nIEEE conference. Currently, he is doing research on data science, \\nmachine learning, cryptography and cyber security . \\n \\n \\nMd. Emanul Haque Rafi  received his bachelor  of science  degree in \\ncomputer science and engineering from  the electrical and computer \\nengineering department of  North So uth University, Bangladesh. Emanul \\nwas born in Dhaka, captial city of Bangladesh. His primary research \\ninterest includes data science  and management , machine learning, deep \\nlearning, and natural language processing . \\n \\nEvan Alam  has a bachelor ’s degree in co mputer science and engineering \\nfrom electrical and computer engineering department of North South \\nUniversity, Bangladesh.  He was an active member of the Computer & \\nEngineering  Club of North South University during his undergraduate \\nstudy.  Currently, his primary research interests are computer vision, data \\nscience, machine learning, and computer network security . \\n \\nJournal of Advances in Information Technology Vol. 13, No. 6, December 2022\\n660Riasat Khan  received a B.Sc. degree in Electrical and Electronic \\nEngineering from the Islamic University of Technology, Bangladesh, in \\n2010. He completed his M.Sc. and Ph.D. degrees in Electrical \\nEngineering from New Mexico State University, Las Cruces, USA, in \\n2018. Currently, Dr. Khan is working as an Assistant Profes sor in the \\nDepartment of Electrical and Computer Engineering at North South \\nUniversity, Dhaka, Bangladesh. His research interests include biomedical \\nengineering, cardiac electrophysiology and computational \\nbioelectromagnetics.  \\nJournal of Advances in Information Technology Vol. 13, No. 6, December 2022\\n661\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "text=extract_text_from_pdf(newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moksha's Lappy\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, render_template\n",
    "import requests\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Replace with your actual API keys\n",
    "GOOGLE_FACT_CHECK_API_KEY = 'YOUR_GOOGLE_FACT_CHECK_API_KEY'\n",
    "MEDIA_BIAS_API_KEY = 'YOUR_MEDIA_BIAS_API_KEY'\n",
    "\n",
    "def google_fact_check(query):\n",
    "    url = f\"https://factchecktools.googleapis.com/v1alpha1/claims:search?query={query}&key={GOOGLE_FACT_CHECK_API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()\n",
    "\n",
    "def media_bias_check(article):\n",
    "    # Hypothetical endpoint for Media Bias Fact Check\n",
    "    url = f\"https://api.mediabiasfactcheck.com/v1/check?article={article}&key={MEDIA_BIAS_API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()\n",
    "\n",
    "def fetch_similar_articles(article_title):\n",
    "    url = f\"https://newsapi.org/v2/everything?q={article_title}&apiKey=YOUR_NEWS_API_KEY\"\n",
    "    response = requests.get(url)\n",
    "    articles = response.json().get('articles', [])\n",
    "    return articles[:5]  # Return top 5 similar articles\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/check_article', methods=['POST'])\n",
    "def check_article():\n",
    "    data = request.json\n",
    "    article_text = data.get('article_text', '')\n",
    "\n",
    "    # Perform Google Fact Check\n",
    "    google_results = google_fact_check(article_text)\n",
    "\n",
    "    # Perform Media Bias Check\n",
    "    media_bias_results = media_bias_check(article_text)\n",
    "\n",
    "    # Fetch similar articles\n",
    "    similar_articles = fetch_similar_articles(article_text)\n",
    "\n",
    "    # Combine results\n",
    "    results = {\n",
    "        'google_fact_check': google_results,\n",
    "        'media_bias_check': media_bias_results,\n",
    "        'similar_articles': similar_articles\n",
    "    }\n",
    "\n",
    "    return jsonify(results)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
